{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vRd0ZT5V4Mc",
        "outputId": "9a342517-4f91-46e5-9ea4-d66046d7f361"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Collecting datasets\n",
            "  Downloading datasets-4.4.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Collecting pyarrow>=21.0.0 (from datasets)\n",
            "  Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
            "Downloading datasets-4.4.0-py3-none-any.whl (511 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.5/511.5 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyarrow, datasets\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 18.1.0\n",
            "    Uninstalling pyarrow-18.1.0:\n",
            "      Successfully uninstalled pyarrow-18.1.0\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
            "pylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-4.4.0 pyarrow-22.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "import numpy as np\n",
        "import torch"
      ],
      "metadata": {
        "id": "osL6Bu6iUJT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Controlla se è disponibile una GPU (MOLTO raccomandato)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Utilizzo del dispositivo: {device}\")"
      ],
      "metadata": {
        "id": "DYX2TdW-UMhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. CARICARE I SET DI DATI ---\n",
        "try:\n",
        "    train_df = pd.read_csv(\"train_set.csv\")\n",
        "    val_df = pd.read_csv(\"validation_set.csv\")\n",
        "    test_df = pd.read_csv(\"test_set.csv\")\n",
        "except FileNotFoundError:\n",
        "    print(\"ERRORE: File di set non trovati.\")\n",
        "    print(\"Esegui prima 'split_data.py'.\")\n",
        "    exit()\n",
        "\n",
        "# Converti i DataFrame di Pandas in 'Dataset' di Hugging Face\n",
        "ds = DatasetDict({\n",
        "    'train': Dataset.from_pandas(train_df),\n",
        "    'validation': Dataset.from_pandas(val_df),\n",
        "    'test': Dataset.from_pandas(test_df)\n",
        "})\n",
        "\n",
        "print(f\"Dataset caricati:\\n{ds}\")"
      ],
      "metadata": {
        "id": "jC8bW7sVUShU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. CARICARE TOKENIZER E MODELLO ---\n",
        "MODEL_NAME = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\"\n",
        "\n",
        "# Carica il Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Carica il Modello\n",
        "# num_labels=2 (classe 0 e classe 1)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
        "model.to(device)  # Sposta il modello sulla GPU se disponibile"
      ],
      "metadata": {
        "id": "ThKCSa_oUfcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. TOKENIZZARE I DATI ---\n",
        "def tokenize_function(examples):\n",
        "    # 'truncation=True' taglia i testi più lunghi di 512 token\n",
        "    # 'padding=\"max_length\"' aggiunge token finti fino a 512\n",
        "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "\n",
        "# Applica la tokenizzazione a tutti i set in parallelo\n",
        "tokenized_datasets = ds.map(tokenize_function, batched=True)\n",
        "\n",
        "# Rimuovi la colonna 'text' (non più necessaria) e formatta per il training\n",
        "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
        "tokenized_datasets.set_format(\"torch\")\n",
        "\n"
      ],
      "metadata": {
        "id": "4AGHwis0Uh7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afd8cd4c"
      },
      "source": [
        "import transformers\n",
        "import datasets\n",
        "\n",
        "print(f\"Transformers version: {transformers.__version__}\")\n",
        "print(f\"Datasets version: {datasets.__version__}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- 4. DEFINIRE LE METRICHE DI VALUTAZIONE ---\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    f1 = f1_score(labels, predictions, average='binary')\n",
        "    precision = precision_score(labels, predictions, average='binary')\n",
        "    recall = recall_score(labels, predictions, average='binary')\n",
        "\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }"
      ],
      "metadata": {
        "id": "TlFSE_HAUkhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. DEFINIRE GLI ARGOMENTI DI TRAINING ---\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./polyphenol_classifier\",  # Cartella dove salvare il modello\n",
        "    eval_strategy=\"epoch\",  # Valuta alla fine di ogni epoca\n",
        "    save_strategy=\"epoch\",  # Salva il modello alla fine di ogni epoca\n",
        "    num_train_epochs=3,  # 3 epoche sono un buon punto di partenza\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    learning_rate=2e-5,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=True,  # Carica il modello migliore alla fine\n",
        "    metric_for_best_model=\"f1\",  # Scegli il modello migliore in base all'F1-score\n",
        "    report_to=\"none\"  # Disabilita il logging online (wandb)\n",
        ")"
      ],
      "metadata": {
        "id": "EcbRdY6rUnOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 6. CREARE IL TRAINER ---\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "fN-KzkrZUpkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 7. AVVIARE IL TRAINING (FINE-TUNING) ---\n",
        "print(\"\\n--- INIZIO FINE-TUNING MODELLO ---\")\n",
        "trainer.train()\n",
        "print(\"--- FINE-TUNING COMPLETATO ---\")"
      ],
      "metadata": {
        "id": "4Atb10iyUuW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 8. VALUTAZIONE FINALE SUL TEST SET ---\n",
        "print(\"\\n--- VALUTAZIONE SUL TEST SET (DATI MAI VISTI) ---\")\n",
        "test_results = trainer.evaluate(eval_dataset=tokenized_datasets[\"test\"])\n",
        "\n",
        "print(f\"Accuracy sul Test Set: {test_results['eval_accuracy']:.4f}\")\n",
        "print(f\"F1-score sul Test Set: {test_results['eval_f1']:.4f}\")\n",
        "print(f\"Precision sul Test Set: {test_results['eval_precision']:.4f}\")\n",
        "print(f\"Recall sul Test Set: {test_results['eval_recall']:.4f}\")"
      ],
      "metadata": {
        "id": "Sxm1sKPBUxKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- OTTENIMENTO DEI LOGITS E DELLE PROBABILITÀ ---\")\n",
        "predictions = trainer.predict(tokenized_datasets[\"test\"])\n",
        "\n",
        "logits = predictions.predictions   # array di dimensione [num_samples, 2]\n",
        "labels = predictions.label_ids     # etichette vere (0 o 1)\n",
        "\n",
        "# Converti i logits in probabilità con softmax\n",
        "probs = torch.softmax(torch.tensor(logits), dim=1)\n",
        "\n",
        "# Estrai la probabilità della classe \"rilevante\" (indice 1)\n",
        "relevance_scores = probs[:, 1].numpy()\n",
        "\n",
        "# Mostra qualche esempio\n",
        "for i in range(5):\n",
        "    print(f\"Esempio {i+1}:\")\n",
        "    print(f\"  Logits grezzi: {logits[i]}\")\n",
        "    print(f\"  Probabilità rilevante: {relevance_scores[i]:.4f}\")\n",
        "    print(f\"  Etichetta vera: {labels[i]}\")"
      ],
      "metadata": {
        "id": "k4gwoVsLlkg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Salva i risultati finali su un file\n",
        "with open(\"test_results.txt\", \"w\") as f:\n",
        "    f.write(str(test_results))\n",
        "\n",
        "# Salva il modello finale\n",
        "trainer.save_model(\"./polyphenol_classifier_final\")\n",
        "print(\"Modello finale salvato in './polyphenol_classifier_final'\")"
      ],
      "metadata": {
        "id": "KgyHuAo4U2zv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r modello_finito.zip ./polyphenol_classifier_final"
      ],
      "metadata": {
        "id": "kJe6r5m9r9va"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import average_precision_score, ndcg_score\n",
        "import numpy as np\n",
        "\n",
        "# --- METRICHE RANKING-BASED ---\n",
        "map_score = average_precision_score(labels, relevance_scores)\n",
        "\n",
        "def mean_reciprocal_rank(y_true, y_score):\n",
        "    sorted_indices = np.argsort(-y_score)\n",
        "    for rank, idx in enumerate(sorted_indices, start=1):\n",
        "        if y_true[idx] == 1:\n",
        "            return 1.0 / rank\n",
        "    return 0.0\n",
        "\n",
        "mrr_score = mean_reciprocal_rank(labels, relevance_scores)\n",
        "ndcg = ndcg_score([labels], [relevance_scores])\n",
        "\n",
        "print(\"\\n--- Ranking-based Metrics ---\")\n",
        "print(f\"MAP: {map_score:.4f}\")\n",
        "print(f\"MRR: {mrr_score:.4f}\")\n",
        "print(f\"nDCG: {ndcg:.4f}\")"
      ],
      "metadata": {
        "id": "0D64zie7mfrG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}